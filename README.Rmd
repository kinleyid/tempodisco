---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# tempodisco

<!-- badges: start -->
[![R-CMD-check](https://github.com/kinleyid/tempodisco/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/kinleyid/tempodisco/actions/workflows/R-CMD-check.yaml)
[![codecov](https://codecov.io/github/kinleyid/tempodisco/graph/badge.svg?token=CCQXS3SNGB)](https://codecov.io/github/kinleyid/tempodisco)
<!-- badges: end -->

`tempodisco` is an R package for working with delay discounting data.

## Installation

You can install tempodisco from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("kinleyid/tempodisco")
```

## Example usage

```{r}
library(tempodisco)
```


### Modeling indifference point data

To compute indifference points from an adjusting amount procedure, we can use the `adj_amt_indiffs` function:

```{r}
data("adj_amt_sim") # Load simulated data from an adjusting amounts procedure
indiff_data <- adj_amt_indiffs(adj_amt_sim)
head(indiff_data)
```

This returns a data frame containing the delays and corresponding indifference points. The function `td_ipm` can then be used to identify the best-fitting discount function (according to the Bayesian information criterion) from any subset of the following options:

```{r child="man/fragments/predefined-discount-functions.Rmd"}
```

For example:

```{r}
mod <- td_ipm(data = indiff_data, discount_function = c('exponential', 'hyperbolic', 'nonlinear-time-hyperbolic'))
print(mod)
```

From here, we can extract useful information about the model and visualize it

```{r}
plot(mod)
print(coef(mod)) # k value
print(BIC(mod)) # Bayesian information criterion
```

### Modeling binary choice data

A traditional method of modeling binary choice data is to compute a value of $k$ using the scoring method introduced for the Kirby Monetary Choice Questionnaire:

```{r}
data("td_bc_single_ptpt")
```

```{r child="man/fragments/kirby-scoring.Rmd"}
```

Another option is to use the logistic regression method of Wileyto et al., where we can solve for the $k$ value of the hyperbolic discount function in terms of the regression coefficients:

```{r child="man/fragments/wileyto-scoring.Rmd"}
```

We can extend this approach to a number of other discount functions using the `method` argument to `td_bclm`:

```{r child="man/fragments/linear-models.Rmd"}
```

By setting `method = "all"` (the default), `td_bclm` tests all of the above models and returns the best-fitting one, according to the Bayesian information criterion:

```{r}
mod <- td_bclm(td_bc_single_ptpt, model = 'all')
print(mod)
```

We can explore an even wider range of discount functions using nonlinear modeling with `td_bcnm`. When `discount_function = "all"` (the default), all of the following models are tested and the best-fitting one (according to the Bayesian information criterion) is returned:

```{r child="man/fragments/predefined-discount-functions.Rmd"}
```

```{r}
mod <- td_bcnm(td_bc_single_ptpt, discount_function = 'all')
plot(mod, log = 'x', verbose = F, p_lines = c(0.05, 0.95))
```

### Drift diffusion models

To model reaction times using a drift diffusion model, we can use `td_ddm` (here, for speed, we are starting the optimization near optimal values for this dataset):

```{r}
ddm <- td_ddm(td_bc_single_ptpt, discount_function = 'exponential',
              v_par_starts = 0.01,
              beta_par_starts = 0.5,
              alpha_par_starts = 3.5,
              tau_par_starts = 0.9)
print(ddm)
```

Following [Peters & D'Esposito (2020)](https://doi.org/10.1371/journal.pcbi.1007615), we can apply a sigmoidal transform to the drift rate $\delta$ to improve model fit using the argument `drift_transform = "sigmoid"`:

$$\delta' = v_\text{max} \left(\frac{2}{1 + e^{-\delta}} - 1\right)$$

```{r}
ddm_sig <- td_ddm(td_bc_single_ptpt, discount_function = 'exponential',
              drift_transform = 'sigmoid',
              v_par_starts = 0.01,
              beta_par_starts = 0.5,
              alpha_par_starts = 3.5,
              tau_par_starts = 0.9)
coef(ddm_sig)
```

This introduces a new variable `max_abs_drift` ($v_\text{max}$ in the equation above) controlling the absolute value of the maximum drift rate.

We can use the model to predict reaction times and compare them to the actual data:

```{r}
pred_rts <- predict(ddm_sig, type = 'rt')
cor.test(pred_rts, td_bc_single_ptpt$rt)
```

We can also plot reaction times against the model's predictions:

```{r}
plot(ddm_sig, type = 'rt', confint = 0.8)
```

### The "model-free" discount function

In addition to the discount functions listed above, we can fit a "model-free" discount function to the data, meaning we fit each indifference point independently. This enables us to, first, test whether a participant exhibits non-systematic discounting according to the Johnson & Bickel criteria:

```{r child="man/fragments/j-b-criteria.Rmd"}
```

```{r}
mod <- td_bcnm(td_bc_single_ptpt, discount_function = 'model-free')
print(nonsys(mod)) # Model violates neither criterion; no non-systematic discounting detected
```

We can also measure the model-free area under the curve (AUC), a useful model-agnostic measure of discounting.

```{r}
print(AUC(mod))
```
